{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkedIn Job Postings Analysis\n",
    "\n",
    "Not completed yet! Things written here are subject to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/09 14:29:29 WARN Utils: Your hostname, plasma resolves to a loopback address: 127.0.1.1; using 192.168.1.21 instead (on interface wlan0)\n",
      "23/10/09 14:29:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/09 14:29:41 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        # local = single thread\n",
    "        # local[*] = max threads\n",
    "        .master('local[*]')\n",
    "        .appName('Spark Test App')\n",
    "        # Hive must to be enabled to connect from Metabase\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='file:/run/media/dhika/Windows/Users/Dhika/Documents/Projects/Proto/Python/Spark/spark-warehouse')]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# View current tables and databases (if any)\n",
    "print(spark.catalog.listDatabases())\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from source files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "files = [\n",
    "    'data/job_postings',\n",
    "    'data/company_details/companies',\n",
    "    'data/company_details/company_industries',\n",
    "    'data/company_details/company_specialities',\n",
    "    'data/company_details/employee_counts',\n",
    "    'data/job_details/benefits',\n",
    "    'data/job_details/job_industries',\n",
    "    'data/job_details/job_skills'\n",
    "]\n",
    "\n",
    "headers = {}\n",
    "\n",
    "# It seems that PySpark can't read complex CSV files correctly\n",
    "# Though Pandas seems able to read them just fine\n",
    "# As workaround, I use Pandas to export them to JSON first\n",
    "for file in files:\n",
    "    if os.path.isfile(f'{file}.csv'):\n",
    "        # Force all column types as string\n",
    "        # Auto conversion is inaccurate\n",
    "        csv = pd.read_csv(f'{file}.csv', dtype = str)\n",
    "        csv.to_json(f'{file}.json', orient = 'records')\n",
    "        # Save the column order for later\n",
    "        headers[f'{file}.json'] = list(csv.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get table overview and join related tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* data/job_postings.json *\n",
      "['job_id', 'company_id', 'title', 'description', 'max_salary', 'med_salary', 'min_salary', 'pay_period', 'formatted_work_type', 'location', 'applies', 'original_listed_time', 'remote_allowed', 'views', 'job_posting_url', 'application_url', 'application_type', 'expiry', 'closed_time', 'formatted_experience_level', 'skills_desc', 'listed_time', 'posting_domain', 'sponsored', 'work_type', 'currency', 'compensation_type']\n",
      "\n",
      "* data/company_details/companies.json *\n",
      "['company_id', 'name', 'description', 'company_size', 'state', 'country', 'city', 'zip_code', 'address', 'url']\n",
      "\n",
      "* data/company_details/company_industries.json *\n",
      "['company_id', 'industry']\n",
      "\n",
      "* data/company_details/company_specialities.json *\n",
      "['company_id', 'speciality']\n",
      "\n",
      "* data/company_details/employee_counts.json *\n",
      "['company_id', 'employee_count', 'follower_count', 'time_recorded']\n",
      "\n",
      "* data/job_details/benefits.json *\n",
      "['job_id', 'inferred', 'type']\n",
      "\n",
      "* data/job_details/job_industries.json *\n",
      "['job_id', 'industry_id']\n",
      "\n",
      "* data/job_details/job_skills.json *\n",
      "['job_id', 'skill_abr']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get table columns overview\n",
    "# Kind of useful to see relationship between tables\n",
    "for key, value in headers.items():\n",
    "    print(f'* {key} *')\n",
    "    print(value)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/08 01:07:38 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "job_postings = 'data/job_postings.json'\n",
    "df_jp = spark.read.json(job_postings)\n",
    "\n",
    "# Reading CSV directly using PySpark may cause issues\n",
    "# Single multiline row can be mistreated as multiple rows\n",
    "\"\"\" # header = get column name from CSV header\n",
    "# inferSchema = auto detect column data type when possible\n",
    "df_jp = (\n",
    "    spark.read.options(\n",
    "        header = True,\n",
    "        inferSchema = True\n",
    "    ).csv('data/job_postings.csv')\n",
    ") \"\"\"\n",
    "\n",
    "# Restore custom column order (the default is ascending)\n",
    "df_jp = df_jp.select(*headers[job_postings])\n",
    "\n",
    "# Print column names and types\n",
    "# df_jp.printSchema()\n",
    "\n",
    "print(df_jp.count())\n",
    "print(df_jp.distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[INVALID_TEMP_OBJ_REFERENCE] Cannot create the persistent object `spark_catalog`.`default`.`job_postings` of the type VIEW because it references to the temporary object `job_postings` of the type VIEW. Please make the temporary object `job_postings` persistent, or make the persistent object `spark_catalog`.`default`.`job_postings` temporary.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/dhika/Documents/Projects/Proto/Python/Spark/main.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dhika/Documents/Projects/Proto/Python/Spark/main.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_jp\u001b[39m.\u001b[39mcreateOrReplaceTempView(\u001b[39m\"\u001b[39m\u001b[39mjob_postings\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dhika/Documents/Projects/Proto/Python/Spark/main.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# We can also use \"select distinct\" to return only unique rows,\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dhika/Documents/Projects/Proto/Python/Spark/main.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# but all rows are already unique based on previous result\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dhika/Documents/Projects/Proto/Python/Spark/main.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m spark\u001b[39m.\u001b[39;49msql(\u001b[39m'\u001b[39;49m\u001b[39mCREATE VIEW job_postings AS SELECT * FROM job_postings\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mshow()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dhika/Documents/Projects/Proto/Python/Spark/main.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# spark.sql('SELECT * FROM job_postings').show()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dhika/Documents/Projects/Proto/Python/Spark/main.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# spark.sql('SELECT COUNT(*) FROM job_postings').show()\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/Proto/Python/Spark/.venv/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonUtils\u001b[39m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m (args \u001b[39mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsparkSession\u001b[39m.\u001b[39;49msql(sqlQuery, litArgs), \u001b[39mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(kwargs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Projects/Proto/Python/Spark/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/Projects/Proto/Python/Spark/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [INVALID_TEMP_OBJ_REFERENCE] Cannot create the persistent object `spark_catalog`.`default`.`job_postings` of the type VIEW because it references to the temporary object `job_postings` of the type VIEW. Please make the temporary object `job_postings` persistent, or make the persistent object `spark_catalog`.`default`.`job_postings` temporary."
     ]
    }
   ],
   "source": [
    "# Treat current CSV as temporary SQL table\n",
    "# You can then execute SQL commands on this table\n",
    "df_jp.createOrReplaceTempView(\"job_postings\")\n",
    "\n",
    "# We can also use \"select distinct\" to return only unique rows,\n",
    "# but all rows are already unique based on previous result\n",
    "spark.sql('CREATE VIEW job_postings AS SELECT * FROM job_postings').show()\n",
    "# spark.sql('SELECT * FROM job_postings').show()\n",
    "# spark.sql('SELECT COUNT(*) FROM job_postings').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Unimportant columns\n",
    "# filter_cols = [\n",
    "#     'description',\n",
    "#     'original_listed_time',\n",
    "#     'application_url',\n",
    "#     'job_posting_url',\n",
    "#     'expiry',\n",
    "#     'closed_time',\n",
    "#     'listed_time',\n",
    "#     'posting_domain',\n",
    "#     'sponsored',\n",
    "#     'work_type'\n",
    "# ]\n",
    "\n",
    "# # FIXME The \"except\" SQL query doesn't work\n",
    "# # Using pythonic way \"not in\" as workaround\n",
    "# filter_cols = [ i for i in df_jp.schema.names if i not in filter_cols ]\n",
    "# filter_cols = ','.join(filter_cols)\n",
    "\n",
    "# spark.sql(f'SELECT {filter_cols} FROM job_postings').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "\"\"\"CREATE TABLE jobs (\n",
    "    id Int,\n",
    "    company String,\n",
    "    industry String,\n",
    "    title String,\n",
    "    experience String,\n",
    "    salary Int,\n",
    "    currency String,\n",
    "    work_type String,\n",
    "    location String,\n",
    "    benefit String\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>company_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>max_salary</th>\n",
       "      <th>med_salary</th>\n",
       "      <th>min_salary</th>\n",
       "      <th>pay_period</th>\n",
       "      <th>formatted_work_type</th>\n",
       "      <th>location</th>\n",
       "      <th>...</th>\n",
       "      <th>expiry</th>\n",
       "      <th>closed_time</th>\n",
       "      <th>formatted_experience_level</th>\n",
       "      <th>skills_desc</th>\n",
       "      <th>listed_time</th>\n",
       "      <th>posting_domain</th>\n",
       "      <th>sponsored</th>\n",
       "      <th>work_type</th>\n",
       "      <th>currency</th>\n",
       "      <th>compensation_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3586162459</td>\n",
       "      <td>69642092</td>\n",
       "      <td>Teradata Developer</td>\n",
       "      <td>Duration: 6-12+ Months\\nOverview of Role:Indiv...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Contract</td>\n",
       "      <td>United States</td>\n",
       "      <td>...</td>\n",
       "      <td>1.70E+12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.69E+12</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTRACT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3690692186</td>\n",
       "      <td>61242</td>\n",
       "      <td>Seasonal Payroll/Data Entry Clerk</td>\n",
       "      <td>Universal Screen Arts' specialty is marketing ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Temporary</td>\n",
       "      <td>Hudson, OH</td>\n",
       "      <td>...</td>\n",
       "      <td>1.71E+12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.69E+12</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>TEMPORARY</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3691795980</td>\n",
       "      <td>7573454</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Job Description:\\n• Design, develop, and launc...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Contract</td>\n",
       "      <td>United States</td>\n",
       "      <td>...</td>\n",
       "      <td>1.70E+12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.69E+12</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTRACT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3692302089</td>\n",
       "      <td>37768</td>\n",
       "      <td>Data Scientist/ Product Analyst</td>\n",
       "      <td>Looking for candidates with 4+ years’ experien...</td>\n",
       "      <td>80</td>\n",
       "      <td>None</td>\n",
       "      <td>70</td>\n",
       "      <td>HOURLY</td>\n",
       "      <td>Contract</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>...</td>\n",
       "      <td>1.70E+12</td>\n",
       "      <td>None</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>None</td>\n",
       "      <td>1.69E+12</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTRACT</td>\n",
       "      <td>USD</td>\n",
       "      <td>BASE_SALARY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3692363778</td>\n",
       "      <td>2474970</td>\n",
       "      <td>Data Analytics Consultant</td>\n",
       "      <td>About the CompanyDiLytics is a leading Informa...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Sacramento, CA</td>\n",
       "      <td>...</td>\n",
       "      <td>1.70E+12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.69E+12</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>3701369746</td>\n",
       "      <td>39203</td>\n",
       "      <td>Data Scientist / Operations Research Analyst</td>\n",
       "      <td>LinQuest is seeking a Data Scientist / Operati...</td>\n",
       "      <td>160000</td>\n",
       "      <td>None</td>\n",
       "      <td>100000</td>\n",
       "      <td>YEARLY</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Colorado Springs, CO</td>\n",
       "      <td>...</td>\n",
       "      <td>1.70E+12</td>\n",
       "      <td>None</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>None</td>\n",
       "      <td>1.69E+12</td>\n",
       "      <td>recruiting2.ultipro.com</td>\n",
       "      <td>0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>USD</td>\n",
       "      <td>BASE_SALARY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>3701371901</td>\n",
       "      <td>2848937</td>\n",
       "      <td>Data Engineering Product Lead (Hybrid)</td>\n",
       "      <td>Job Description\\nThe Data Engineering Product ...</td>\n",
       "      <td>304500</td>\n",
       "      <td>None</td>\n",
       "      <td>193440</td>\n",
       "      <td>YEARLY</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>West Point, PA</td>\n",
       "      <td>...</td>\n",
       "      <td>1.70E+12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.69E+12</td>\n",
       "      <td>jobs.msd.com</td>\n",
       "      <td>1</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>USD</td>\n",
       "      <td>BASE_SALARY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>3701372446</td>\n",
       "      <td>2113831</td>\n",
       "      <td>OCM Data Analyst (Remote)</td>\n",
       "      <td>GovCIO is looking for an experienced Data Anal...</td>\n",
       "      <td>88000</td>\n",
       "      <td>None</td>\n",
       "      <td>85000</td>\n",
       "      <td>YEARLY</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Fairfax, VA</td>\n",
       "      <td>...</td>\n",
       "      <td>1.70E+12</td>\n",
       "      <td>None</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>None</td>\n",
       "      <td>1.69E+12</td>\n",
       "      <td>careers-govcio.icims.com</td>\n",
       "      <td>0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>USD</td>\n",
       "      <td>BASE_SALARY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>3701372789</td>\n",
       "      <td>163578</td>\n",
       "      <td>Senior Supply Chain Data Analyst</td>\n",
       "      <td>Job Description:\\nOur Sr. Supply Chain Data An...</td>\n",
       "      <td>108000</td>\n",
       "      <td>None</td>\n",
       "      <td>76000</td>\n",
       "      <td>YEARLY</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Irvine, CA</td>\n",
       "      <td>...</td>\n",
       "      <td>1.70E+12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.69E+12</td>\n",
       "      <td>edwards.wd5.myworkdayjobs.com</td>\n",
       "      <td>1</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>USD</td>\n",
       "      <td>BASE_SALARY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>3701373490</td>\n",
       "      <td>26205</td>\n",
       "      <td>An, Database Developer</td>\n",
       "      <td>Description\\nAn, Database Developer AFA / ISD ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Oklahoma, United States</td>\n",
       "      <td>...</td>\n",
       "      <td>1.70E+12</td>\n",
       "      <td>None</td>\n",
       "      <td>Associate</td>\n",
       "      <td>None</td>\n",
       "      <td>1.69E+12</td>\n",
       "      <td>www.hrapply.com</td>\n",
       "      <td>0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         job_id company_id                                         title  \\\n",
       "0    3586162459   69642092                            Teradata Developer   \n",
       "1    3690692186      61242             Seasonal Payroll/Data Entry Clerk   \n",
       "2    3691795980    7573454                                 Data Engineer   \n",
       "3    3692302089      37768              Data Scientist/ Product Analyst    \n",
       "4    3692363778    2474970                     Data Analytics Consultant   \n",
       "..          ...        ...                                           ...   \n",
       "376  3701369746      39203  Data Scientist / Operations Research Analyst   \n",
       "377  3701371901    2848937        Data Engineering Product Lead (Hybrid)   \n",
       "378  3701372446    2113831                     OCM Data Analyst (Remote)   \n",
       "379  3701372789     163578              Senior Supply Chain Data Analyst   \n",
       "380  3701373490      26205                        An, Database Developer   \n",
       "\n",
       "                                           description max_salary med_salary  \\\n",
       "0    Duration: 6-12+ Months\\nOverview of Role:Indiv...       None       None   \n",
       "1    Universal Screen Arts' specialty is marketing ...       None       None   \n",
       "2    Job Description:\\n• Design, develop, and launc...       None       None   \n",
       "3    Looking for candidates with 4+ years’ experien...         80       None   \n",
       "4    About the CompanyDiLytics is a leading Informa...       None       None   \n",
       "..                                                 ...        ...        ...   \n",
       "376  LinQuest is seeking a Data Scientist / Operati...     160000       None   \n",
       "377  Job Description\\nThe Data Engineering Product ...     304500       None   \n",
       "378  GovCIO is looking for an experienced Data Anal...      88000       None   \n",
       "379  Job Description:\\nOur Sr. Supply Chain Data An...     108000       None   \n",
       "380  Description\\nAn, Database Developer AFA / ISD ...       None       None   \n",
       "\n",
       "    min_salary pay_period formatted_work_type                 location  ...  \\\n",
       "0         None       None            Contract            United States  ...   \n",
       "1         None       None           Temporary               Hudson, OH  ...   \n",
       "2         None       None            Contract            United States  ...   \n",
       "3           70     HOURLY            Contract        San Francisco, CA  ...   \n",
       "4         None       None           Full-time           Sacramento, CA  ...   \n",
       "..         ...        ...                 ...                      ...  ...   \n",
       "376     100000     YEARLY           Full-time     Colorado Springs, CO  ...   \n",
       "377     193440     YEARLY           Full-time           West Point, PA  ...   \n",
       "378      85000     YEARLY           Full-time              Fairfax, VA  ...   \n",
       "379      76000     YEARLY           Full-time               Irvine, CA  ...   \n",
       "380       None       None           Full-time  Oklahoma, United States  ...   \n",
       "\n",
       "       expiry closed_time formatted_experience_level skills_desc listed_time  \\\n",
       "0    1.70E+12        None                       None        None    1.69E+12   \n",
       "1    1.71E+12        None                       None        None    1.69E+12   \n",
       "2    1.70E+12        None                       None        None    1.69E+12   \n",
       "3    1.70E+12        None           Mid-Senior level        None    1.69E+12   \n",
       "4    1.70E+12        None                       None        None    1.69E+12   \n",
       "..        ...         ...                        ...         ...         ...   \n",
       "376  1.70E+12        None                Entry level        None    1.69E+12   \n",
       "377  1.70E+12        None                       None        None    1.69E+12   \n",
       "378  1.70E+12        None                Entry level        None    1.69E+12   \n",
       "379  1.70E+12        None                       None        None    1.69E+12   \n",
       "380  1.70E+12        None                  Associate        None    1.69E+12   \n",
       "\n",
       "                    posting_domain sponsored  work_type currency  \\\n",
       "0                             None         0   CONTRACT     None   \n",
       "1                             None         0  TEMPORARY     None   \n",
       "2                             None         0   CONTRACT     None   \n",
       "3                             None         0   CONTRACT      USD   \n",
       "4                             None         0  FULL_TIME     None   \n",
       "..                             ...       ...        ...      ...   \n",
       "376        recruiting2.ultipro.com         0  FULL_TIME      USD   \n",
       "377                   jobs.msd.com         1  FULL_TIME      USD   \n",
       "378       careers-govcio.icims.com         0  FULL_TIME      USD   \n",
       "379  edwards.wd5.myworkdayjobs.com         1  FULL_TIME      USD   \n",
       "380                www.hrapply.com         0  FULL_TIME     None   \n",
       "\n",
       "    compensation_type  \n",
       "0                None  \n",
       "1                None  \n",
       "2                None  \n",
       "3         BASE_SALARY  \n",
       "4                None  \n",
       "..                ...  \n",
       "376       BASE_SALARY  \n",
       "377       BASE_SALARY  \n",
       "378       BASE_SALARY  \n",
       "379       BASE_SALARY  \n",
       "380              None  \n",
       "\n",
       "[381 rows x 27 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM job_postings WHERE LOWER(title) LIKE \"%data%\"').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables = [ i.name for i in spark.catalog.listTables() ]\n",
    "# print(tables)\n",
    "\n",
    "# for table in tables:\n",
    "#     spark.catalog.dropTempView(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
